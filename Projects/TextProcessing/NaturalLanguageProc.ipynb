{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# natural language toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-27 06:52:52.368 Python[955:8385] WARNING: Secure coding is not enabled for restorable state! Enable secure coding by implementing NSApplicationDelegate.applicationSupportsSecureRestorableState: and returning YES.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()\n",
    "\n",
    "# install allcorpara in that popup window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A corpus - a large collection of text documents - statements\n",
    "# data given by nltk, we focus on algo.\n",
    "# import brown corpus below\n",
    "from nltk.corpus import brown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mType:\u001b[0m           CategorizedTaggedCorpusReader\n",
      "\u001b[0;31mString form:\u001b[0m    <CategorizedTaggedCorpusReader in '/Users/kritimalik/nltk_data/corpora/brown'>\n",
      "\u001b[0;31mFile:\u001b[0m           /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/nltk/corpus/reader/tagged.py\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "A reader for part-of-speech tagged corpora whose documents are\n",
      "divided into categories based on their file identifiers.\n",
      "\u001b[0;31mInit docstring:\u001b[0m\n",
      "Initialize the corpus reader.  Categorization arguments\n",
      "(``cat_pattern``, ``cat_map``, and ``cat_file``) are passed to\n",
      "the ``CategorizedCorpusReader`` constructor.  The remaining arguments\n",
      "are passed to the ``TaggedCorpusReader``."
     ]
    }
   ],
   "source": [
    "brown?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adventure',\n",
       " 'belles_lettres',\n",
       " 'editorial',\n",
       " 'fiction',\n",
       " 'government',\n",
       " 'hobbies',\n",
       " 'humor',\n",
       " 'learned',\n",
       " 'lore',\n",
       " 'mystery',\n",
       " 'news',\n",
       " 'religion',\n",
       " 'reviews',\n",
       " 'romance',\n",
       " 'science_fiction']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.categories() # categories for which data is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(brown.categories())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = brown.sents(categories='adventure') # Get sentences from brown corpus with adventure label/class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Dan', 'Morgan', 'told', 'himself', 'he', 'would', 'forget', 'Ann', 'Turner', '.'], ['He', 'was', 'well', 'rid', 'of', 'her', '.'], ...]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Given a sentence, classify it in a topic\n",
    "# Output is list of lists - list of sentences\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4637"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)\n",
    "# 4637 number of sentences belonging to adventure category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dan Morgan told himself he would forget Ann Turner .'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This is the First sentence in adventure category\n",
    "' '.join(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of words pipeline\n",
    "-  Get the data/corpus\n",
    "-  Tokenisation, Stopward Removal\n",
    "-  Stemming\n",
    "-  Building a vocab\n",
    "-  Vectorization\n",
    "-  Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenisation\n",
    "\n",
    "- For a large doc on sports - this doc will get broken to sentences and each sentence gets broken down to words. Extract the smallest token out of the document. Extract all the words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopward Removal\n",
    "- was, had etc words can be discarded - not meaningful or relevant for Machine Learning task - they doesn't describe the sentence. \n",
    "- Objective is to get smaller dataset - we get lesser number of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming / Lemmatization\n",
    "- Convert different forms of a word into its base form - example - running as run, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a Vocab\n",
    "- List of unique words after all pre-processing as mentioned above. Assign a number to each unique word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorization\n",
    "- Prep vector for each sentence\n",
    "- Vector size is equal to Vocab size\n",
    "- If a word is at 50th index in Vocab, then at 50th index in Vector, set frequency of this word = 1 and so on\n",
    "Note: size of vector is larger than sentence size coz vector is of size as that of vocab.\n",
    "- Feed these vectors i.e. vector for each sentence to the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why Bag of Words?\n",
    "\n",
    "- Order of words occuring in a sentence is not important.\n",
    "- Put all important words in a sentence inside a bag,\n",
    "- Get those words out of the bag and then make a vetor of it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 - TOKENISATION AND STOPWORD REMOVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# below is a multiline string\n",
    "\n",
    "document = \"\"\"It was a very pleasant day. The weather was cool and there were light showers. I went to the market to buy some fruits\"\"\"\n",
    "\n",
    "sentence = \"send all the 50 documents related to chapters 1, 2, 3 at prateek@cb.com\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It was a very pleasant day.', 'The weather was cool and there were light showers.', 'I went to the market to buy some fruits']\n",
      "It was a very pleasant day.\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "sents = sent_tokenize(document)\n",
    "print(sents) # we get a list of 3 items / 3 sentences\n",
    "print(sents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Difference between python split and word_tokenize below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['send',\n",
       " 'all',\n",
       " 'the',\n",
       " '50',\n",
       " 'documents',\n",
       " 'related',\n",
       " 'to',\n",
       " 'chapters',\n",
       " '1,',\n",
       " '2,',\n",
       " '3',\n",
       " 'at',\n",
       " 'prateek@cb.com']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence.split() # custom tokenization - we can improve it further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['send',\n",
       " 'all',\n",
       " 'the',\n",
       " '50',\n",
       " 'documents',\n",
       " 'related',\n",
       " 'to',\n",
       " 'chapters',\n",
       " '1',\n",
       " ',',\n",
       " '2',\n",
       " ',',\n",
       " '3',\n",
       " 'at',\n",
       " 'prateek',\n",
       " '@',\n",
       " 'cb.com']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = word_tokenize(sentence) # this is a fixed implementation\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 - STOP WORD REMOVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "sw = set(stopwords.words('english')) # these are common stopwords\n",
    "sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text, stopwords):\n",
    "    useful_words = [w for w in text if (w not in stopwords or w == 'not')] # this is list comprehension\n",
    "    return useful_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', ' ', ' ', 'n', ' ', 'b', 'h', 'e', 'r', 'e', ' ', 'b', 'u', ' ', 'h', 'e', 'r', ' ', 'v', 'e', 'r', ' ', 'u', 'c', 'h']\n"
     ]
    }
   ],
   "source": [
    "text = \"I am not bothered about her very much\"\n",
    "useful_text = remove_stopwords(text, sw)\n",
    "print(useful_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "['I', 'am', 'not', 'bothered', 'about', 'her', 'very', 'much']\n",
      "['I', 'not', 'bothered', 'much']\n"
     ]
    }
   ],
   "source": [
    "# send as list of words, text is a string - \n",
    "# Note: Iteration over string by default goes over every character - So, do tokenization of text first before sending it to remove_stopwords\n",
    "text_fixed = \"I am not bothered about her very much\".split()\n",
    "print(type(text_fixed))\n",
    "print(text_fixed)\n",
    "useful_text_fixed = remove_stopwords((text_fixed), sw)\n",
    "print(useful_text_fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'not' in sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "['send', 'all', 'the', '50', 'documents', 'related', 'to', 'chapters', '1,', '2,', '3', 'at', 'prateek@cb.com']\n",
      "['send', '50', 'documents', 'related', 'chapters', '1,', '2,', '3', 'prateek@cb.com']\n"
     ]
    }
   ],
   "source": [
    "sentence_new = sentence.split()\n",
    "print(type(sentence_new))\n",
    "print(sentence_new)\n",
    "useful_sentence_new = remove_stopwords((sentence_new), sw)\n",
    "print(useful_sentence_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 - Tokenisation using Regular Expression\n",
    "- Practice regex at regexpal.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objective: if we aim to \"Extract only words\"\n",
    "# 1, 2, 3 - got captured as tokens in above example but they aren't relevant or very meaningful\n",
    "# - we need to remove them - but there can be many such numbers and each number can be a big number\n",
    "# we can't put all such numbers in stopwords list because numbers can be infinite and we cant put a hard code logic to remove every number\n",
    "# Thus stopword removal is not good to remove 1,2,3 .. \n",
    "# Then, regex helps in tokenisation instead, both python split and remove_stopwords wont work in this usecase.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['send', 'all', 'the', 'documents', 'related', 'to', 'chapters', 'at', 'prateek@cb.com']\n",
      "['send', 'documents', 'related', 'chapters', 'prateek@cb.com']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer('[a-zA-Z@.]+')\n",
    "useful_text_regex = tokenizer.tokenize(sentence)\n",
    "print(useful_text_regex)\n",
    "print(remove_stopwords(useful_text_regex, sw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lemmatization takes more time as compared to stemming because it finds meaningful word/ representation. \n",
    "- Stemming just needs to get a base word and therefore takes less time.\n",
    "\n",
    "- Stemming has its application in Sentiment Analysis while Lemmatization has its application in Chatbots, human-answering\n",
    "\n",
    "\n",
    "- stemming the word ‘Caring‘ would return ‘Car‘. Stemming is used in case of large dataset where performance is an issue.\n",
    "- lemmatizing the word ‘Caring‘ would return ‘Care‘. Lemmatization is computationally expensive since it involves look-up tables and what not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jump\n",
      "jump\n",
      "care\n",
      "love\n",
      "love\n",
      "###############################\n",
      "jump\n",
      "jump\n",
      "care\n",
      "love\n",
      "love\n",
      "###############################\n",
      "jump\n",
      "jump\n",
      "car\n",
      "lov\n",
      "lov\n",
      "###############################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jumping\n",
      "jump\n",
      "Caring\n",
      "lovely\n",
      "loving\n"
     ]
    }
   ],
   "source": [
    "text_to_stem = \"\"\"Foxes love to make jumps. The quick brown fox was seen jumping over the lovely dog from a 6ft feet high wall\"\"\"\n",
    "\n",
    "# NLTK provides 3 types of stemmers - Snowball Stemmer, Porter, Lancaster Stemmer\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer, PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "print(ps.stem('jumping'))\n",
    "print(ps.stem('jumps'))\n",
    "print(ps.stem('Caring'))\n",
    "print(ps.stem('lovely'))\n",
    "print(ps.stem('loving'))\n",
    "print('###############################')\n",
    "\n",
    "ps = SnowballStemmer('english')\n",
    "print(ps.stem('jumping'))\n",
    "print(ps.stem('jumps'))\n",
    "print(ps.stem('Caring'))\n",
    "print(ps.stem('lovely'))\n",
    "print(ps.stem('loving'))\n",
    "\n",
    "print('###############################')\n",
    "ps = LancasterStemmer()\n",
    "print(ps.stem('jumping'))\n",
    "print(ps.stem('jumps'))\n",
    "print(ps.stem('Caring'))\n",
    "print(ps.stem('lovely'))\n",
    "print(ps.stem('loving'))\n",
    "\n",
    "# Lemmetization\n",
    "print('###############################')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wn = WordNetLemmatizer()\n",
    "print(wn.lemmatize('jumping'))\n",
    "print(wn.lemmatize('jumps'))\n",
    "print(wn.lemmatize('Caring'))\n",
    "print(wn.lemmatize('lovely'))\n",
    "print(wn.lemmatize('loving'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Vectorization - Convert a given sentence into vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample Corpus - Containe 4 Documents, each document can have 1 or more sentences\n",
    "\n",
    "corpus = [\n",
    "    'Indian cricket team will wins World Cup, says Capt. Virat Kohli. \\\n",
    "    World cup will be held at Sri Lanka. ',\n",
    "    'We will win next Lok Sabha Elections, says confident Indian PM',\n",
    "    'The nobel laurate won the hearts of the people.',\n",
    "    'The movie Raazi is an exciting Indian Spy thriller based upon a real story'\n",
    "]\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each document gets converted to a feature vector. We can then perform classification on the documents - to find type(sports, movies etc) of document. \n",
    "# Here, we will get 4 feature vectors in above example\n",
    "# Collection of unique words across all documents - Vocab\n",
    "# So, we will first build Vocab and then we create a vector of Vocab size for each document. \n",
    "# Suppose word 'Indian' occurs in Vocab at 7th index , \n",
    "#    so, inside feature vector of the document(sentence) having 'Indian' word, we will put 1 (or the computed frequency) at 7th index inside feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x42 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 47 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer()\n",
    "\n",
    "# fit_transform: Learn(fit) the dictionary from given data and transform this data into vectorized form\n",
    "vectorized_corpus = cv.fit_transform(corpus)\n",
    "\n",
    "# vectorized_corpus is a sparse matrix\n",
    "vectorized_corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mType:\u001b[0m        csr_matrix\n",
      "\u001b[0;31mString form:\u001b[0m\n",
      "(0, 12)\t1\n",
      "           (0, 6)\t1\n",
      "           (0, 31)\t1\n",
      "           (0, 37)\t2\n",
      "           (0, 39)\t1\n",
      "           (0, 41)\t2\n",
      "           (0, 7)\t2\n",
      "           (0, 27)\t1\n",
      "           ( <...> 3)\t1\n",
      "           (3, 0)\t1\n",
      "           (3, 9)\t1\n",
      "           (3, 28)\t1\n",
      "           (3, 33)\t1\n",
      "           (3, 2)\t1\n",
      "           (3, 34)\t1\n",
      "           (3, 25)\t1\n",
      "           (3, 30)\t1\n",
      "\u001b[0;31mFile:\u001b[0m        /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/scipy/sparse/_csr.py\n",
      "\u001b[0;31mDocstring:\u001b[0m  \n",
      "Compressed Sparse Row matrix\n",
      "\n",
      "This can be instantiated in several ways:\n",
      "    csr_array(D)\n",
      "        with a dense matrix or rank-2 ndarray D\n",
      "\n",
      "    csr_array(S)\n",
      "        with another sparse matrix S (equivalent to S.tocsr())\n",
      "\n",
      "    csr_array((M, N), [dtype])\n",
      "        to construct an empty matrix with shape (M, N)\n",
      "        dtype is optional, defaulting to dtype='d'.\n",
      "\n",
      "    csr_array((data, (row_ind, col_ind)), [shape=(M, N)])\n",
      "        where ``data``, ``row_ind`` and ``col_ind`` satisfy the\n",
      "        relationship ``a[row_ind[k], col_ind[k]] = data[k]``.\n",
      "\n",
      "    csr_array((data, indices, indptr), [shape=(M, N)])\n",
      "        is the standard CSR representation where the column indices for\n",
      "        row i are stored in ``indices[indptr[i]:indptr[i+1]]`` and their\n",
      "        corresponding values are stored in ``data[indptr[i]:indptr[i+1]]``.\n",
      "        If the shape parameter is not supplied, the matrix dimensions\n",
      "        are inferred from the index arrays.\n",
      "\n",
      "Attributes\n",
      "----------\n",
      "dtype : dtype\n",
      "    Data type of the matrix\n",
      "shape : 2-tuple\n",
      "    Shape of the matrix\n",
      "ndim : int\n",
      "    Number of dimensions (this is always 2)\n",
      "nnz\n",
      "    Number of stored values, including explicit zeros\n",
      "data\n",
      "    CSR format data array of the matrix\n",
      "indices\n",
      "    CSR format index array of the matrix\n",
      "indptr\n",
      "    CSR format index pointer array of the matrix\n",
      "has_sorted_indices\n",
      "    Whether indices are sorted\n",
      "\n",
      "Notes\n",
      "-----\n",
      "\n",
      "Sparse matrices can be used in arithmetic operations: they support\n",
      "addition, subtraction, multiplication, division, and matrix power.\n",
      "\n",
      "Advantages of the CSR format\n",
      "  - efficient arithmetic operations CSR + CSR, CSR * CSR, etc.\n",
      "  - efficient row slicing\n",
      "  - fast matrix vector products\n",
      "\n",
      "Disadvantages of the CSR format\n",
      "  - slow column slicing operations (consider CSC)\n",
      "  - changes to the sparsity structure are expensive (consider LIL or DOK)\n",
      "\n",
      "Canonical Format\n",
      "    - Within each row, indices are sorted by column.\n",
      "    - There are no duplicate entries.\n",
      "\n",
      "Examples\n",
      "--------\n",
      "\n",
      ">>> import numpy as np\n",
      ">>> from scipy.sparse import csr_array\n",
      ">>> csr_array((3, 4), dtype=np.int8).toarray()\n",
      "array([[0, 0, 0, 0],\n",
      "       [0, 0, 0, 0],\n",
      "       [0, 0, 0, 0]], dtype=int8)\n",
      "\n",
      ">>> row = np.array([0, 0, 1, 2, 2, 2])\n",
      ">>> col = np.array([0, 2, 2, 0, 1, 2])\n",
      ">>> data = np.array([1, 2, 3, 4, 5, 6])\n",
      ">>> csr_array((data, (row, col)), shape=(3, 3)).toarray()\n",
      "array([[1, 0, 2],\n",
      "       [0, 0, 3],\n",
      "       [4, 5, 6]])\n",
      "\n",
      ">>> indptr = np.array([0, 2, 3, 6])\n",
      ">>> indices = np.array([0, 2, 2, 0, 1, 2])\n",
      ">>> data = np.array([1, 2, 3, 4, 5, 6])\n",
      ">>> csr_array((data, indices, indptr), shape=(3, 3)).toarray()\n",
      "array([[1, 0, 2],\n",
      "       [0, 0, 3],\n",
      "       [4, 5, 6]])\n",
      "\n",
      "Duplicate entries are summed together:\n",
      "\n",
      ">>> row = np.array([0, 1, 2, 0])\n",
      ">>> col = np.array([0, 1, 1, 0])\n",
      ">>> data = np.array([1, 2, 4, 8])\n",
      ">>> csr_array((data, (row, col)), shape=(3, 3)).toarray()\n",
      "array([[9, 0, 0],\n",
      "       [0, 2, 0],\n",
      "       [0, 4, 0]])\n",
      "\n",
      "As an example of how to construct a CSR matrix incrementally,\n",
      "the following snippet builds a term-document matrix from texts:\n",
      "\n",
      ">>> docs = [[\"hello\", \"world\", \"hello\"], [\"goodbye\", \"cruel\", \"world\"]]\n",
      ">>> indptr = [0]\n",
      ">>> indices = []\n",
      ">>> data = []\n",
      ">>> vocabulary = {}\n",
      ">>> for d in docs:\n",
      "...     for term in d:\n",
      "...         index = vocabulary.setdefault(term, len(vocabulary))\n",
      "...         indices.append(index)\n",
      "...         data.append(1)\n",
      "...     indptr.append(len(indices))\n",
      "...\n",
      ">>> csr_array((data, indices, indptr), dtype=int).toarray()\n",
      "array([[2, 1, 0, 0],\n",
      "       [0, 1, 1, 1]])"
     ]
    }
   ],
   "source": [
    "vectorized_corpus?\n",
    "# There is no strict definition regarding the proportion of zero-value elements for a matrix to qualify as sparse \n",
    "# but a common criterion is that the number of non-zero elements is roughly equal to the number of rows or columns. \n",
    "# By contrast, if most of the elements are non-zero, the matrix is considered dense.\n",
    "\n",
    "# Ex- \"(0, 6) \t1\"\n",
    "# Inside 0th document, 1st word is at 6th index in vocab and has a frequency of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 1 1 0 1 2 0 0 0 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1\n",
      "  0 2 0 1 0 2]\n",
      " [0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 1 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0\n",
      "  1 1 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 3 0 0 0\n",
      "  0 0 0 0 1 0]\n",
      " [1 0 1 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 1 1 0\n",
      "  0 0 0 0 0 0]]\n",
      "document 1 vector below\n",
      "[[0 1 0 1 1 0 1 2 0 0 0 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1\n",
      "  0 2 0 1 0 2]]\n"
     ]
    }
   ],
   "source": [
    "# convert from sparse matrix to array -\n",
    "print(vectorized_corpus.toarray())\n",
    "print(\"document 1 vector below\")\n",
    "print(vectorized_corpus[0].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'indian': 12,\n",
       " 'cricket': 6,\n",
       " 'team': 31,\n",
       " 'will': 37,\n",
       " 'wins': 39,\n",
       " 'world': 41,\n",
       " 'cup': 7,\n",
       " 'says': 27,\n",
       " 'capt': 4,\n",
       " 'virat': 35,\n",
       " 'kohli': 14,\n",
       " 'be': 3,\n",
       " 'held': 11,\n",
       " 'at': 1,\n",
       " 'sri': 29,\n",
       " 'lanka': 15,\n",
       " 'we': 36,\n",
       " 'win': 38,\n",
       " 'next': 19,\n",
       " 'lok': 17,\n",
       " 'sabha': 26,\n",
       " 'elections': 8,\n",
       " 'confident': 5,\n",
       " 'pm': 23,\n",
       " 'the': 32,\n",
       " 'nobel': 20,\n",
       " 'laurate': 16,\n",
       " 'won': 40,\n",
       " 'hearts': 10,\n",
       " 'of': 21,\n",
       " 'people': 22,\n",
       " 'movie': 18,\n",
       " 'raazi': 24,\n",
       " 'is': 13,\n",
       " 'an': 0,\n",
       " 'exciting': 9,\n",
       " 'spy': 28,\n",
       " 'thriller': 33,\n",
       " 'based': 2,\n",
       " 'upon': 34,\n",
       " 'real': 25,\n",
       " 'story': 30}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of unique words in entire corpus(vector size same as vocab size, all those positions will be 0 in a sentence where given word is not present) -\n",
      "(4, 42) 42 42\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Length of unique words in entire corpus(vector size same as vocab size, all those positions will be 0 in a sentence where given word is not present) -')\n",
    "print(vectorized_corpus.shape, len(vectorized_corpus.toarray()[0]), len(cv.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numbers in 2nd doc: [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 3 0 0 0 0\n",
      " 0 0 0 1 0], numbers.shape = (42,)\n",
      "reversed mapping - words in document 2 are - [array(['hearts', 'laurate', 'nobel', 'of', 'people', 'the', 'won'],\n",
      "      dtype='<U9')]\n",
      "type(s) - <class 'list'>, type(apple) - <class 'list'>, joined apple = buy sell\n"
     ]
    }
   ],
   "source": [
    "# Reverse mapping - convert a list of numbers in vector into the sentence.\n",
    "vectorized_corpus_arr = vectorized_corpus.toarray()\n",
    "len(vectorized_corpus_arr[0])\n",
    "numbers = vectorized_corpus_arr[2] # 2nd document taken\n",
    "print(f'numbers in 2nd doc: {numbers}, numbers.shape = {numbers.shape}')\n",
    "s = cv.inverse_transform(numbers.reshape(1,-1)) # one row with all features\n",
    "# Note: this is a bag of \"unordered\" unique words.\n",
    "print(f'reversed mapping - words in document 2 are - {s}')\n",
    "# ' '.join(s)\n",
    "apple = ['buy', 'sell']\n",
    "joined_apple = ' '.join(apple)\n",
    "print(f'type(s) - {type(s)}, type(apple) - {type(apple)}, joined apple = {joined_apple}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Vectorization with Stopword removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "send all the 50 documents related to chapters 1, 2, 3 at prateek@cb.com\n",
      "['send', 'documents', 'related', 'chapters', 'prateek@cb.com']\n"
     ]
    }
   ],
   "source": [
    "def myTokenizer(document):\n",
    "    words = tokenizer.tokenize(document.lower())\n",
    "    words = remove_stopwords(words, sw)\n",
    "    return words\n",
    "\n",
    "print(sentence)\n",
    "print(myTokenizer(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 33) 33 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# so, with custom tokenizer myTokenizer, compute vectorized_corpus again with reduced number of unique words\n",
    "cv_custom_tok = CountVectorizer(tokenizer=myTokenizer)\n",
    "vectorized_corpus_custom = cv_custom_tok.fit_transform(corpus)\n",
    "print(vectorized_corpus_custom.shape, len(vectorized_corpus_custom.toarray()[0]), len(cv_custom_tok.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['indian', 'cricket', 'team', 'wins', 'world', 'cup', 'says',\n",
       "        'capt.', 'virat', 'kohli.', 'held', 'sri', 'lanka.'], dtype='<U9'),\n",
       " array(['indian', 'says', 'win', 'next', 'lok', 'sabha', 'elections',\n",
       "        'confident', 'pm'], dtype='<U9'),\n",
       " array(['nobel', 'laurate', 'hearts', 'people.'], dtype='<U9'),\n",
       " array(['indian', 'movie', 'raazi', 'exciting', 'spy', 'thriller', 'based',\n",
       "        'upon', 'real', 'story'], dtype='<U9')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_custom_tok.inverse_transform(vectorized_corpus_custom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for test data, we will not learn new vocab. We will use the vocab learnt on training data.\n",
    "test_corpus = [\n",
    "    'Indian cricket rock!'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_custom_tok.transform(test_corpus).toarray() # Only transform method gets invoked in case of test data. Coz we dont want to learn vocab for test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. More ways to create Features\n",
    "- Unigram\n",
    "- Bigrams\n",
    "- Trigrams\n",
    "- n-grams\n",
    "- TF-IDF Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_1 =['this is good movie']\n",
    "sent_2 =['this is not good movie']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_improve_features = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this is good movie']\n",
      "this is good movie\n"
     ]
    }
   ],
   "source": [
    "print(sent_1)\n",
    "print(sent_1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 0, 1],\n",
       "       [1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unigram - has every word as feature\n",
    "docs = [sent_1[0], sent_2[0]] # Club both sentences into a single document\n",
    "cv_improve_features.fit_transform(docs).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion in Classifier if \"not good\" is NOT considered a single feature in second sentence of above example\n",
    "# In bigrams - we consider every pair of words as a single feature \n",
    "# It will increase the size of vector though"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_improve_features_bigram = CountVectorizer(ngram_range=(2,2)) # bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 0, 1],\n",
       "       [1, 0, 1, 1, 1]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_improve_features_bigram.fit_transform(docs).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this is': 4, 'is good': 1, 'good movie': 0, 'is not': 2, 'not good': 3}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_improve_features_bigram.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this is good': 3,\n",
       " 'is good movie': 0,\n",
       " 'this is not': 4,\n",
       " 'is not good': 1,\n",
       " 'not good movie': 2}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_improve_features_trigram = CountVectorizer(ngram_range=(3,3)) # trigram\n",
    "cv_improve_features_trigram.fit_transform(docs).toarray()\n",
    "cv_improve_features_trigram.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 11,\n",
       " 'is': 2,\n",
       " 'good': 0,\n",
       " 'movie': 7,\n",
       " 'this is': 12,\n",
       " 'is good': 3,\n",
       " 'good movie': 1,\n",
       " 'this is good': 13,\n",
       " 'is good movie': 4,\n",
       " 'not': 8,\n",
       " 'is not': 5,\n",
       " 'not good': 9,\n",
       " 'this is not': 14,\n",
       " 'is not good': 6,\n",
       " 'not good movie': 10}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ngrams  - combine unigram, bigram, etc features - essentially to learn all features in the vocab provided the given range in ngram_range\n",
    "cv_improve_features_ngrams = CountVectorizer(ngram_range=(1,3)) # Ngrams\n",
    "cv_improve_features_ngrams.fit_transform(docs).toarray()\n",
    "cv_improve_features_ngrams.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. TFIDF Normalisation (Term Frequency - Inverse document frequency)\n",
    "\n",
    "\n",
    "- Avoid features that occur very often, because they contain less information - like \"the\" feature across documents and in general \n",
    "- Information decreases as the number of occurances increases across different type of documents - \n",
    "- if \"cricket\" word present in all documents, relevance of cricket as feature decreases. However, if \"cricket\" word present only in 1 doc but occurs 20 times, it's imp feature\n",
    "- So we define another term - term-document-frequency which associate a weight with every term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this is good movie', 'this was good movie', 'this is not good movie']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_1_tfidf = 'this is good movie'\n",
    "sent_2_tfidf = 'this was good movie'\n",
    "sent_3_tfidf = 'this is not good movie'\n",
    "\n",
    "corpus_tfidf = [sent_1_tfidf, sent_2_tfidf, sent_3_tfidf]\n",
    "corpus_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "term_freq \n",
    "       = tf(term_t, document_d)\n",
    "       = calculate term_freq(\"this\", sent_1)\n",
    "       = \"this\" occurs 1 times in this first document, \"good\" occurs 1 times in this first document, etc\n",
    "\n",
    "idf\n",
    "  = idf(term_t, document_d) = log (N / (1 + count(t, D))) \n",
    "  Note: - count(t, D) - how many times term t appeared across all documents\n",
    "        -  total number of documents\n",
    "\n",
    "Compute tfidf for \"good\" in document 1 -\n",
    "term_freq = 1\n",
    "idf = log(3/(1 + 3)) ~ log(1) ~ 0\n",
    "- which means - \"DECREASE\" the weight of \"good\" is 0 and has least importance as \"good\" occurs in all 3 docuents\n",
    "\n",
    "But if sent_1 has 'this is awesome movie':\n",
    "-\n",
    "then tfidf for \"awesome\" in document 1\n",
    "term_freq = 1\n",
    "idf = log(3/(1 + 1)) ~ log(3) ~ word \"awesome\" will just get higher weight\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Thus instead of just assigning a count like 1,2,3 to features in the vector for a given sentence, we will give weights to the features which is between 0-1\n",
    "\n",
    "- Weights containing more info ll have weight close to 1\n",
    "\n",
    "- Weights containing less info ll have weight close to 0\n",
    "\n",
    "- Ex - word \"not\" in sent_3 will get higher weight - shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.46333427, 0.59662724, 0.46333427, 0.        , 0.46333427,\n",
       "        0.        ],\n",
       "       [0.41285857, 0.        , 0.41285857, 0.        , 0.41285857,\n",
       "        0.69903033],\n",
       "       [0.3645444 , 0.46941728, 0.3645444 , 0.61722732, 0.3645444 ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "vc = tfidf.fit_transform(corpus_tfidf)\n",
    "\n",
    "vc.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 4, 'is': 1, 'good': 0, 'movie': 2, 'was': 5, 'not': 3}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.vocabulary_\n",
    "\n",
    "# these are tfidf features normalized by their weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
